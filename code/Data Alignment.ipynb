{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.robust import mad\n",
    "import math\n",
    "import datetime\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visits Data - Product (Target Variable) - US All Product (Web-App) Visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visits_product:\n",
    "\n",
    "- Loads product visit data from multiple Excel files (spanning 2021 to 2024).\n",
    "- Processes the data by converting date columns, filtering dates, and replacing certain values (like '-' with NaN).\n",
    "- Combines data into a single DataFrame.\n",
    "\n",
    "### visits_product_aggregate:\n",
    "\n",
    "- Aggregates the product visit data by date, summing up visits and goals, and averaging the \"% of Goal\".\n",
    "- Merges weather impact data back into the aggregated results and recalculates the \"% of Goal\".\n",
    "\n",
    "### title_case:\n",
    "\n",
    "- Converts a string to title case (capitalizing each word) and removes spaces.\n",
    "\n",
    "### get_model_product_visits:\n",
    "\n",
    "- Processes and aggregates product visit data by product and platform.\n",
    "- Renames columns to indicate visits and goals per product.\n",
    "- Merges product-level data with overall product visit aggregation.\n",
    "- Ensures weather impact rating is properly formatted and merged with the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "- US All Product (Web-App) Visits 2021-10.17.23 - REVISED 1.4.24.xlsx - Contains Visits data from 2021-10-17 to 2023-10-01 (yyyy-mm-dd)\n",
    "- US All Product (Web-App) Visits 10.1.23-12.31.23.xlsx - Contains Visits data from 2023-10-01 to 2023-12-31 (yyyy-mm-dd)\n",
    "- US All Product (Web-App) Visits 1.1.24-5.31.24.xlsx - Contains Visits data from 2024-01-01 to 2024-05-31 (yyyy-mm-dd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visits_product(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes product visit data from multiple Excel files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed product visit data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from Excel files\n",
    "    df_visits_2021_to_2023 = pd.read_excel(file_path + \"US All Product (Web-App) Visits 2021-10.17.23 - REVISED 1.4.24.xlsx\")\n",
    "    df_visits_2023_oct = pd.read_excel(file_path + \"US All Product (Web-App) Visits 10.1.23-12.31.23.xlsx\")\n",
    "    df_visits_2024 = pd.read_excel(file_path + \"US All Product (Web-App) Visits 1.1.24-5.31.24.xlsx\")\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_visits_2021_to_2023['Date'] = pd.to_datetime(df_visits_2021_to_2023['Date'])\n",
    "    df_visits_2023_oct['Date'] = pd.to_datetime(df_visits_2023_oct['Date'])\n",
    "    df_visits_2024['Date'] = pd.to_datetime(df_visits_2024['Date'])\n",
    "    \n",
    "    # Filter data for df_v1 to include dates before 2023-10-01\n",
    "    df_visits_2021_to_2023 = df_visits_2021_to_2023[df_visits_2021_to_2023['Date'] < '2023-10-01']\n",
    "\n",
    "    # Concatenate the dataframes\n",
    "    df_visits_product = pd.concat([df_visits_2021_to_2023, df_visits_2023_oct, df_visits_2024])\n",
    "    \n",
    "    # Replace '-' with NaN in relevant columns\n",
    "    df_visits_product['% of Goal'] = df_visits_product['% of Goal'].replace('-', pd.NA)\n",
    "    df_visits_product['US Weather Impact Rating'] = df_visits_product['US Weather Impact Rating'].replace('-', pd.NA)\n",
    "\n",
    "    return df_visits_product\n",
    "\n",
    "def visits_product_aggregate(file_path):\n",
    "    \"\"\"\n",
    "    Aggregates product visit data by date.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with aggregated product visit data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_visits_product = visits_product(file_path)\n",
    "    \n",
    "    # Group by 'Date' and aggregate values\n",
    "    df_visits_grp = df_visits_product.groupby('Date').agg({\n",
    "        'Value': 'sum',\n",
    "        'Goal': 'sum',\n",
    "        '% of Goal': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge the weather impact rating back into the grouped data\n",
    "    df_visits_grp = pd.merge(\n",
    "        df_visits_grp, \n",
    "        df_visits_product[['Date', 'US Weather Impact Rating']].drop_duplicates().sort_values('Date').reset_index(drop=True), \n",
    "        on='Date'\n",
    "    )\n",
    "\n",
    "    # Recalculate '% of Goal'\n",
    "    df_visits_grp['% of Goal'] = df_visits_grp['Value'] / df_visits_grp['Goal']\n",
    "    df_visits_grp['% of Goal'] = df_visits_grp['% of Goal'].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df_visits_grp\n",
    "\n",
    "def title_case(var):\n",
    "    \"\"\"\n",
    "    Converts a string to title case while removing spaces.\n",
    "\n",
    "    Parameters:\n",
    "    - var: str, the string to be converted\n",
    "\n",
    "    Returns:\n",
    "    - str, the title-cased string with spaces removed\n",
    "    \"\"\"\n",
    "    \n",
    "    return ''.join(x for x in var.title() if not x.isspace())\n",
    "\n",
    "def get_model_product_visits(file_path):\n",
    "    \"\"\"\n",
    "    Processes and aggregates product visit data by product and overall.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed and aggregated product visit data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_ = visits_product(file_path)\n",
    "    df_['Date'] = pd.to_datetime(df_['Date'])\n",
    "    \n",
    "    df_visits = pd.DataFrame(columns=['Date'])\n",
    "\n",
    "    # Process each unique product\n",
    "    for ven in df_['Product'].unique():\n",
    "        temp = df_[df_['Product'] == ven]\n",
    "        var = title_case(temp['Platform'].values[0]) + \"_\" + ven\n",
    "        \n",
    "        # Group by 'Date', 'Platform', and 'Product' and sum the values\n",
    "        df_visits_grp = temp.groupby(['Date', 'Platform', 'Product']).sum().reset_index()\n",
    "        df_visits_grp = df_visits_grp.drop(['Platform', 'Product'], axis=1)\n",
    "        \n",
    "        # Rename columns\n",
    "        df_visits_grp = df_visits_grp.rename({\n",
    "            'Value': var + \"_Visits\", \n",
    "            'Goal': var + \"_Goal\"\n",
    "        }, axis=1)\n",
    "        \n",
    "        # Merge with the result dataframe\n",
    "        df_visits = pd.merge(df_visits, df_visits_grp, on='Date', how='outer')\n",
    "\n",
    "    # Aggregate product visits and goals\n",
    "    df_visits_grp = visits_product_aggregate(file_path)\n",
    "    df_visits_grp = df_visits_grp.drop('% of Goal', axis=1)\n",
    "    df_visits_grp = df_visits_grp.rename({\n",
    "        'Value': \"Overall_Product_Visits\", \n",
    "        'Goal': \"Overall_Product_Goal\"\n",
    "    }, axis=1)\n",
    "    \n",
    "    # Merge aggregated data into the result dataframe\n",
    "    df_visits = pd.merge(df_visits, df_visits_grp, on='Date', how='outer')\n",
    "\n",
    "    # Convert 'US Weather Impact Rating' to float\n",
    "    df_visits['US Weather Impact Rating'] = df_visits['US Weather Impact Rating'].astype('float')\n",
    "    \n",
    "    return df_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Organic Search - Impressions/Clicks: Google Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_organic_search_google:\n",
    "\n",
    "- Loads Google search impression data from three Excel files covering different date ranges.\n",
    "- Combines the data from all three files into a single DataFrame.\n",
    "- Converts the 'Date' column into a proper datetime format for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- Search Impressions - Google Only - 6.18.22-10.17.23.xlsx - Contains Organic Search data from 2022-06-18 to 2023-10-17 (yyyy-mm-dd)\n",
    "- Search Impressions - Google Only - 10.18.23-12.31.23 FIXED.xlsx - Contains Organic Search data from 2023-10-18 to 2023-12-31 (yyyy-mm-dd)\n",
    "- Search Impressions - Google Only - 1.1.24-5.31.24.xlsx - Contains Organic Search data from 2024-01-01 to 2024-05-31 (yyyy-mm-dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_organic_search_google(file_path):\n",
    "    \"\"\"\n",
    "    Reads and concatenates Google search impression data from multiple Excel files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with concatenated search impression data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read Excel files from different date ranges\n",
    "    df_organic_2022_to_2023 = pd.read_excel(\n",
    "        file_path + \"Search Impressions - Google Only - 6.18.22-10.17.23.xlsx\", \n",
    "        sheet_name='Dates'\n",
    "    )\n",
    "    df_organic_2023_oct = pd.read_excel(\n",
    "        file_path + \"Search Impressions - Google Only - 10.18.23-12.31.23 FIXED.xlsx\", \n",
    "        sheet_name='Dates'\n",
    "    )\n",
    "    df_organic_2024 = pd.read_excel(\n",
    "        file_path + \"Search Impressions - Google Only - 1.1.24-5.31.24.xlsx\", \n",
    "        sheet_name='Dates'\n",
    "    )\n",
    "\n",
    "    # Concatenate dataframes\n",
    "    df_organic = pd.concat([df_organic_2022_to_2023, df_organic_2023_oct, df_organic_2024])\n",
    "\n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_organic['Date'] = pd.to_datetime(df_organic['Date'])\n",
    "\n",
    "    return df_organic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: SEO Clicks - For Organic Search Clicks Imputation for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seo_pre_processing:\n",
    "\n",
    "- Cleans column names and 'Segment' values: Strips spaces from column names and values.\n",
    "- Merges the data: Combines SEO data from 2021, 2022, and 2023 on the 'Segment' column.\n",
    "- Selects columns: Filters columns based on a date threshold ('2021-08-01').\n",
    "- Transposes the data: Sets 'Segment' as the index, transposes the dataframe, and renames the 'index' column to 'Date'.\n",
    "\n",
    "### seo_get_combined_column:\n",
    "\n",
    "- Combines desktop and mobile SEO clicks: Adds the columns for \"desktop\" and \"mobile web\" SEO clicks into a single combined column.\n",
    "- Selects relevant columns: Keeps only the 'Date' and the combined SEO clicks column.\n",
    "\n",
    "### seo_clicks:\n",
    "\n",
    "- Reads data from Excel files: Loads SEO click data for 2021, 2022, and 2023 from Excel files, skipping the first 6 rows.\n",
    "- Processes the data: **Calls seo_pre_processing** to merge and clean the data, then **seo_get_combined_column** to aggregate the clicks.\n",
    "- Returns the result: Outputs a DataFrame with combined desktop and mobile SEO clicks for the specified date range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "- US Web SEO Visits - 2021.xlsx - Contains SEO data of 2021 \n",
    "- US Web SEO Visits - 2022.xlsx - Contains SEO data of 2022\n",
    "- US Web SEO Visits - 2023 1.1-10.16.xlsx - Contains SEO data of 2023 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seo_pre_processing(df_seo_2021, df_seo_2022, df_seo_2023):\n",
    "    \"\"\"\n",
    "    Pre-processes SEO data from multiple years by merging dataframes, \n",
    "    selecting columns based on a date filter, and transforming the data.\n",
    "\n",
    "    Parameters:\n",
    "    - df_seo_2021: DataFrame, SEO data for 2021\n",
    "    - df_seo_2022: DataFrame, SEO data for 2022\n",
    "    - df_seo_2023: DataFrame, SEO data for 2023\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with combined and processed SEO data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strip spaces from column names and 'Segment' values for all dataframes\n",
    "    df_seo_2021.columns = df_seo_2021.columns.str.strip()\n",
    "    df_seo_2021['Segment'] = df_seo_2021['Segment'].str.strip()\n",
    "    \n",
    "    df_seo_2022.columns = df_seo_2022.columns.str.strip()\n",
    "    df_seo_2022['Segment'] = df_seo_2022['Segment'].str.strip()\n",
    "    \n",
    "    df_seo_2023.columns = df_seo_2023.columns.str.strip()\n",
    "    df_seo_2023['Segment'] = df_seo_2023['Segment'].str.strip()\n",
    "    \n",
    "    # Merge the dataframes on 'Segment' column\n",
    "    df_seo = pd.merge(df_seo_2021, df_seo_2022, on='Segment', how='outer')\n",
    "    df_seo = pd.merge(df_seo, df_seo_2023, on='Segment', how='outer')\n",
    "\n",
    "    # Select columns based on the date filter\n",
    "    df_columns = df_seo.columns\n",
    "    df_columns_to_include = df_columns[df_columns > '2021-08-01']\n",
    "    df_seo = df_seo[df_columns_to_include]\n",
    "\n",
    "    # Set 'Segment' as index and transpose the dataframe\n",
    "    df_seo.set_index('Segment', inplace=True)\n",
    "    df_seo = df_seo.T.reset_index()\n",
    "\n",
    "    # Rename the 'index' column to 'Date'\n",
    "    df_seo = df_seo.rename({'index': 'Date'}, axis=1)\n",
    "\n",
    "    return df_seo\n",
    "\n",
    "def seo_get_combined_column(df_seo):\n",
    "    \"\"\"\n",
    "    Combines desktop and mobile SEO clicks into a single column.\n",
    "\n",
    "    Parameters:\n",
    "    - df_seo: DataFrame, SEO data with separate columns for desktop and mobile clicks\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with combined SEO clicks column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine desktop and mobile SEO clicks into a single column\n",
    "    df_seo['Desktop & Mobile SEO Clicks (Combined)'] = (\n",
    "        df_seo['Organic Search; United States; desktop'] + \n",
    "        df_seo['Organic Search; United States; mobile web']\n",
    "    )\n",
    "\n",
    "    # Select only 'Date' and the combined SEO clicks column\n",
    "    df_seo = df_seo[['Date', 'Desktop & Mobile SEO Clicks (Combined)']]\n",
    "\n",
    "    return df_seo\n",
    "\n",
    "def seo_clicks(file_path):\n",
    "    \"\"\"\n",
    "    Reads and processes SEO click data from multiple Excel files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with combined SEO clicks data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data from Excel files, skipping the first 6 rows\n",
    "    df_seo_2021 = pd.read_excel(file_path + \"US Web SEO Visits - 2021.xlsx\", skiprows=6)\n",
    "    df_seo_2022 = pd.read_excel(file_path + \"US Web SEO Visits - 2022.xlsx\", skiprows=6)\n",
    "    df_seo_2023 = pd.read_excel(file_path + \"US Web SEO Visits - 2023 1.1-10.16.xlsx\", skiprows=6)\n",
    "\n",
    "    # Pre-process the data and get combined SEO clicks\n",
    "    df_seo = seo_pre_processing(df_seo_2021, df_seo_2022, df_seo_2023)\n",
    "    df_seo = seo_get_combined_column(df_seo)\n",
    "\n",
    "    return df_seo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pricing/App Installs (Android and iOS) - App Installs for Android and iOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pricing_android:\n",
    "\n",
    "- Reads Android installation data from an Excel file.\n",
    "- Processes the data by ensuring the 'Date' is in a consistent format and the 'Installs' column is of type float.\n",
    "- Returns the processed DataFrame with Android installs.\n",
    "\n",
    "### pricing_iOS:\n",
    "\n",
    "- Reads iOS installation data from an Excel file.\n",
    "- Similarly processes the 'Date' and 'Installs' columns to ensure consistency.\n",
    "- Returns the processed DataFrame with iOS installs.\n",
    "\n",
    "### pricing_aggregate:\n",
    "\n",
    "- Processes and aggregates installation data for both Android and iOS by calling the previous two functions.\n",
    "- Groups and sums installs by date for both platforms, then merges them into a single DataFrame.\n",
    "- Adds a new column for total installs (Android + iOS) and returns the aggregated result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "- Android Overview Dash Table Exported.xlsx - Contains Android Installs Data\n",
    "- iOS Overview Dash Table Exported.xlsx - Contains iOS Installs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricing_android(file_path):\n",
    "    \"\"\"\n",
    "    Reads and processes Android installation data from an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed Android install data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the Android Overview data from the Excel file\n",
    "    df_android_installs = pd.read_excel(file_path + \"Android Overview Dash Table Exported.xlsx\")\n",
    "    \n",
    "    # Convert the 'Date' column to string format (YYYY-MM-DD)\n",
    "    df_android_installs['Date'] = pd.to_datetime(df_android_installs['Date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Ensure the 'Installs' column is of type float\n",
    "    df_android_installs['Installs'] = df_android_installs['Installs'].astype(float)\n",
    "    \n",
    "    # Convert the 'Date' column back to datetime format\n",
    "    df_android_installs['Date'] = pd.to_datetime(df_android_installs['Date'])\n",
    "    \n",
    "    # Return the processed DataFrame for Android installs\n",
    "    return df_android_installs\n",
    "\n",
    "def pricing_iOS(file_path):\n",
    "    \"\"\"\n",
    "    Reads and processes iOS installation data from an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed iOS install data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the iOS Overview data from the Excel file\n",
    "    df_ios_installs = pd.read_excel(file_path + \"iOS Overview Dash Table Exported.xlsx\")\n",
    "    \n",
    "    # Convert the 'Date' column to string format (YYYY-MM-DD)\n",
    "    df_ios_installs['Date'] = pd.to_datetime(df_ios_installs['Date']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Ensure the 'Installs' column is of type float\n",
    "    df_ios_installs['Installs'] = df_ios_installs['Installs'].astype(float)\n",
    "    \n",
    "    # Convert the 'Date' column back to datetime format\n",
    "    df_ios_installs['Date'] = pd.to_datetime(df_ios_installs['Date'])\n",
    "    \n",
    "    # Return the processed DataFrame for iOS installs\n",
    "    return df_ios_installs\n",
    "\n",
    "def pricing_aggregate(file_path):\n",
    "    \"\"\"\n",
    "    Aggregates installation data for Android and iOS platforms.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with aggregated install data for both platforms\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Android and iOS install data\n",
    "    df_android = pricing_android(file_path)\n",
    "    df_iOS = pricing_iOS(file_path)\n",
    "\n",
    "    # Group by 'Date' and sum the values for both platforms\n",
    "    df_android = df_android.groupby('Date').sum()\n",
    "    df_iOS = df_iOS.groupby('Date').sum()\n",
    "\n",
    "    # Rename 'Installs' column to distinguish Android and iOS\n",
    "    df_android = df_android.rename({'Installs': 'Android_Installs'}, axis=1)\n",
    "    df_iOS = df_iOS.rename({'Installs': 'iOS_Installs'}, axis=1)\n",
    "\n",
    "    # Merge the Android and iOS data on 'Date' and calculate total installs\n",
    "    df_installs = pd.merge(df_android, df_iOS, on='Date', how='outer').reset_index()\n",
    "    df_installs['Total_Installs'] = df_installs['Android_Installs'] + df_installs['iOS_Installs']\n",
    "\n",
    "    return df_installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: User Acquisition - Media Spend, Impression and Clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mkt_media_spend_pre_processing:\n",
    "- This function preprocesses marketing media spend data according to the type of media sheet\n",
    "\n",
    "- Converts Spend and Clicks columns from strings to floats, handling currency symbols and commas.\n",
    "- Renames columns based on specific media sheets to ensure consistency.\n",
    "- Standardizes date formats based on media sheet type, creating a new_date column with formatted dates.\n",
    "- Renames and drops columns as needed, setting the appropriate date column name.\n",
    "\n",
    "\n",
    "### mkt_media_spend_analyis:\n",
    "- This function analyzes and aggregates marketing media spend data\n",
    "\n",
    "- Loads data from various media sheets.\n",
    "- Calls mkt_media_spend_pre_processing to clean and standardize data.\n",
    "- Aggregates data by summing values for each date.\n",
    "- Renames columns to reflect the source and sheet type for consistency.\n",
    "- Combines aggregated data from all sheets into a single DataFrame.\n",
    "\n",
    "### get_model_user_acquisition_mkt_media_spend:\n",
    "- This function processes and aggregates user acquisition marketing media spend data from multiple sources:\n",
    "\n",
    "- Specifies the list of media sources and their corresponding data source names.\n",
    "- Calls mkt_media_spend_analyis to process and aggregate data for each media source.\n",
    "- Returns the combined DataFrame containing aggregated marketing media spend data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "- BL_TWC _ Marketing Spend Data.xlsx - Contains Paid Media Data for User Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkt_media_spend_pre_processing(df_paid_media, paid_media_sheet):\n",
    "    \"\"\"\n",
    "    Pre-processes marketing media spend data based on the media sheet type.\n",
    "\n",
    "    Parameters:\n",
    "    - df_paid_media: DataFrame, marketing media spend data\n",
    "    - paid_media_sheet: str, type of media sheet to apply specific processing rules\n",
    "\n",
    "    Returns:\n",
    "    - df_paid_media: DataFrame, processed marketing media spend data\n",
    "    - date_var: str, name of the date column in the processed DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert 'Spend' column to float if it is in object format with currency symbols\n",
    "    if df_paid_media['Spend'].dtype == 'object':\n",
    "        df_paid_media['Spend'] = (df_paid_media['Spend']\n",
    "                        .str.replace(',', '')\n",
    "                        .str.replace('$', '')\n",
    "                        .astype(float))\n",
    "    \n",
    "    # Rename 'clicks' to 'Clicks' if present\n",
    "    if 'clicks' in df_paid_media.columns:\n",
    "        df_paid_media = df_paid_media.rename({'clicks': 'Clicks'}, axis=1)\n",
    "    \n",
    "    # Clean and convert 'Clicks' column to float if in object format\n",
    "    if 'Clicks' in df_paid_media.columns and df_paid_media['Clicks'].dtype == 'object':\n",
    "        df_paid_media['Clicks'] = (df_paid_media['Clicks']\n",
    "                         .replace({'\\$': ''}, regex=True)\n",
    "                         .replace({'\\,': ''}, regex=True)\n",
    "                         .astype(float))\n",
    "    \n",
    "    # Handle specific cases based on the media sheet\n",
    "    if paid_media_sheet == 'IronSource Sonic':\n",
    "        df_paid_media = df_paid_media.rename({'OS': 'event_date', 'Day': 'OS'}, axis=1)\n",
    "    \n",
    "    if paid_media_sheet == 'Persona.ly':\n",
    "        df_paid_media = df_paid_media[~df_paid_media['Platform'].isna()]\n",
    "    \n",
    "    if 'event_date' in df_paid_media.columns:\n",
    "        df_paid_media = df_paid_media.rename({'event_date': 'Day'}, axis=1)\n",
    "    \n",
    "    # Process dates for Persona.ly\n",
    "    if paid_media_sheet == 'Persona.ly':\n",
    "        df_paid_media['new_date'] = df_paid_media['Day']\n",
    "        df_paid_media['format'] = 1\n",
    "        \n",
    "        # Identify and process date formats\n",
    "        df_paid_media.loc[df_paid_media.Day.str.contains('/') == True, 'format'] = 2\n",
    "        df_paid_media.loc[df_paid_media.format == 2, 'new_date'] = pd.to_datetime(\n",
    "            df_paid_media.loc[df_paid_media.format == 2, 'Day'], format='%m/%d/%y').dt.strftime('%Y-%m-%d')\n",
    "        df_paid_media.loc[df_paid_media.Date_Issue_flag == 1, 'new_date'] = pd.to_datetime(\n",
    "            df_paid_media.loc[df_paid_media.Date_Issue_flag == 1, 'Day'], format='%Y-%d-%m %H:%M:%S').dt.strftime('%Y-%m-%d')\n",
    "        df_paid_media.loc[df_paid_media.format == 1, 'new_date'] = pd.to_datetime(\n",
    "            df_paid_media.loc[df_paid_media.format == 1, 'Day'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n",
    "        df_paid_media.loc[df_paid_media.Date_Issue_flag == 1, 'new_date'] = pd.to_datetime(\n",
    "            df_paid_media.loc[df_paid_media.Date_Issue_flag == 1, 'Day'], format='%Y-%d-%m %H:%M:%S').dt.strftime('%Y-%m-%d')\n",
    "        df_paid_media = df_paid_media.drop('Date_Issue_flag', axis=1)\n",
    "    \n",
    "    # Process dates for other media sheets\n",
    "    else:\n",
    "        df_paid_media['format'] = 1\n",
    "        \n",
    "        # Identify and process date formats\n",
    "        df_paid_media.loc[df_paid_media.Day.str.contains('/') == True, 'format'] = 2\n",
    "        df_paid_media.loc[df_paid_media.format == 1, 'new_date'] = pd.to_datetime(\n",
    "            df_paid_media.loc[df_paid_media.format == 1, 'Day'], format='%Y-%d-%m %H:%M:%S').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        if paid_media_sheet == 'Bidease':\n",
    "            pattern = r'\\b\\d{2}/\\d{2}/\\d{2}\\b'\n",
    "            df_paid_media.loc[((df_paid_media.format == 2) & (df_paid_media.Day.str.contains(pattern, regex=True))), 'format'] = 3\n",
    "            df_paid_media.loc[df_paid_media.format == 2, 'new_date'] = pd.to_datetime(\n",
    "                df_paid_media.loc[df_paid_media.format == 2, 'Day'], format='%m/%d/%Y').dt.strftime('%Y-%m-%d')\n",
    "            df_paid_media.loc[df_paid_media.format == 3, 'new_date'] = pd.to_datetime(\n",
    "                df_paid_media.loc[df_paid_media.format == 3, 'Day'], format='%m/%d/%y').dt.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            df_paid_media.loc[df_paid_media.format == 2, 'new_date'] = pd.to_datetime(\n",
    "                df_paid_media.loc[df_paid_media.format == 2, 'Day'], format='%m/%d/%y').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Finalize date column renaming and drop unnecessary columns\n",
    "    if 'event_date' in df_paid_media.columns:\n",
    "        df_paid_media = df_paid_media.rename({'Day': 'event_date'}, axis=1)\n",
    "        date_var = 'event_date'\n",
    "    else:\n",
    "        date_var = 'Day'\n",
    "    \n",
    "    df_paid_media[date_var] = df_paid_media['new_date']\n",
    "    df_paid_media = df_paid_media.drop(['format', 'new_date'], axis=1)\n",
    "    \n",
    "    return df_paid_media, date_var\n",
    "\n",
    "def mkt_media_spend_analyis(file_path, paid_media, data_source_dic):\n",
    "    \"\"\"\n",
    "    Analyzes marketing media spend data by processing and aggregating data from different media sheets.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "    - paid_media: list of str, names of the media sheets to process\n",
    "    - data_source_dic: dict, mapping of media sheet names to data source names\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing the aggregated marketing spend data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_result = pd.DataFrame(columns=['Date'])\n",
    "    \n",
    "    for paid_media_sheet in paid_media:\n",
    "        # Load the data from the appropriate sheet\n",
    "        if paid_media_sheet == 'Persona.ly':\n",
    "            df_paid_media = pd.read_excel(file_path + \"BL_TWC _ Marketing Spend Data.xlsx\", sheet_name='Persona.ly')\n",
    "        else:\n",
    "            df_paid_media = pd.read_excel(file_path + \"BL_TWC _ Marketing Spend Data.xlsx\", sheet_name=paid_media_sheet)\n",
    "        \n",
    "        # Pre-process the data\n",
    "        df_paid_media, date_var = mkt_media_spend_pre_processing(df_paid_media, paid_media_sheet)\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        df_paid_media = df_paid_media.rename({'Day': 'Date', 'event_date': 'Date'}, axis=1)\n",
    "        \n",
    "        # Existing commented code\n",
    "        # platform = list(df_paid_media['Platform'].unique())\n",
    "        # os_type = df_paid_media['OS'].unique()\n",
    "        # temp_os_var = ''\n",
    "        # if len(os_type) == 1:\n",
    "        #     temp_os_var = '_Overall'\n",
    "        # for os in os_type:\n",
    "        #     df_grp = df_paid_media[df_['OS'] == os].reset_index(drop=True)\n",
    "        #     df_grp['Date'] = pd.to_datetime(df_grp['Date'])\n",
    "        #     df_grp = df_grp.groupby(['Date', 'Platform', 'OS']).sum().reset_index()\n",
    "        #     df_grp = df_grp.drop(['Platform', 'OS'], axis=1)\n",
    "        #     df_grp = df_grp.rename(columns={col: data_source_dic[paid_media_sheet] + \"_\" + paid_media_sheet + \"_\" + os + temp_os_var + \"_\" + col if col != 'Date' else col for col in df_grp.columns})\n",
    "        #     df_grp.columns = df_grp.columns.str.replace(' ', '_')\n",
    "        #     df_grp['Date'] = pd.to_datetime(df_grp['Date'])\n",
    "        #     df_result = pd.merge(df_result, df_grp, on='Date', how='outer')\n",
    "        \n",
    "        # if len(os_type) > 1:\n",
    "        #     df_grp = df_paid_media.groupby('Date').sum().reset_index()\n",
    "        #     df_grp['Date'] = pd.to_datetime(df_grp['Date'])\n",
    "        #     df_grp = df_grp.rename(columns={col: data_source_dic[paid_media_sheet] + \"_\" + paid_media_sheet + \"_Overall\" + \"_\" + col if col != 'Date' else col for col in df_grp.columns})\n",
    "        #     df_grp.columns = df_grp.columns.str.replace(' ', '_')\n",
    "        #     df_result = pd.merge(df_result, df_grp, on='Date', how='outer')\n",
    "        \n",
    "        # Aggregate data and rename columns\n",
    "        df_grp = df_paid_media.groupby('Date').sum().reset_index()\n",
    "        df_grp['Date'] = pd.to_datetime(df_grp['Date'])\n",
    "        df_grp = df_grp.rename(columns={col: data_source_dic[paid_media_sheet] + \"_\" + paid_media_sheet + \"_\" + col if col != 'Date' else col for col in df_grp.columns})\n",
    "        df_grp.columns = df_grp.columns.str.replace(' ', '_')\n",
    "        \n",
    "        # Merge aggregated data with the result DataFrame\n",
    "        df_result = pd.merge(df_result, df_grp, on='Date', how='outer')\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def get_model_user_acquisition_mkt_media_spend(file_path):\n",
    "    \"\"\"\n",
    "    Processes and aggregates user acquisition marketing media spend data from multiple sources.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing the aggregated marketing media spend data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the list of paid media sources to process\n",
    "    paid_media = [\n",
    "        'Google', 'IronSource Aura', 'Bidease', 'Digital Turbine',\n",
    "        'Tapjoy', 'Apple Search Ads', 'LiftOff', 'IronSource Sonic',\n",
    "        'Twitter', 'TikTok', 'Vibe', 'Persona.ly'\n",
    "    ]\n",
    "    \n",
    "    # Define the mapping of paid media sources to their respective data source names\n",
    "    data_source_dic = {\n",
    "        'Google': 'SEM',\n",
    "        'IronSource Aura': 'Preload',\n",
    "        'Bidease': 'Programmatic',\n",
    "        'Digital Turbine': 'Preload',\n",
    "        'Tapjoy': 'Programmatic',\n",
    "        'Apple Search Ads': 'SEM',\n",
    "        'LiftOff': 'Programmatic',\n",
    "        'IronSource Sonic': 'Programmatic',\n",
    "        'Twitter': 'PaidSocial',\n",
    "        'TikTok': 'Programmatic',\n",
    "        'Vibe': 'PaidSocial',\n",
    "        'Persona.ly': 'Programmatic'\n",
    "    }\n",
    "    \n",
    "    # Analyze and aggregate marketing media spend data\n",
    "    df_paid_media = mkt_media_spend_analyis(file_path, paid_media, data_source_dic)\n",
    "    \n",
    "    return df_paid_media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Brand - Media Spend, Impression (US Brand Basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US_brand_media_spend_impression:\n",
    "\n",
    "- Reads brand media spend and impression data from three Excel files covering different time periods.\n",
    "- Concatenates the data from these files into a single DataFrame.\n",
    "- Converts the 'Date' column to a datetime format.\n",
    "- Returns the concatenated and processed data.\n",
    "\n",
    "### get_model_brand_media_spend_impression:\n",
    "\n",
    "- Uses the data from **US_brand_media_spend_impression** and prepares it for further analysis.\n",
    "- Aggregates the data by date, summing up the total impressions and media spend for each day.\n",
    "- Returns a DataFrame with the aggregated brand media impressions and spend, grouped by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- US Brand - Basis - 2021-2023.xlsx -  Contains Brand Spend from 2021 to 2023-10-15\n",
    "- US Brand - Basis - 10.16.23-12.31.23.xlsx -  Contains Brand Spend from 2023-10-16 to 2023-12-31\n",
    "- TWC_MMM_Brand_Data File_01.01.24-05.31.24.xlsx -  Contains Brand Spend from 2024-01-01 to 2024-05-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def US_brand_media_spend_impression(file_path):\n",
    "    \"\"\"\n",
    "    Reads and concatenates brand media spend and impression data from multiple Excel files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with concatenated data from all files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read data from multiple Excel files\n",
    "    df_brand_2021_to_2023 = pd.read_excel(file_path + \"US Brand - Basis - 2021-2023.xlsx\")\n",
    "    df_brand_2023_oct = pd.read_excel(file_path + \"US Brand - Basis - 10.16.23-12.31.23.xlsx\")\n",
    "    df_brand_2024 = pd.read_excel(file_path + \"TWC_MMM_Brand_Data File_01.01.24-05.31.24.xlsx\")\n",
    "    \n",
    "    # Concatenate data from all files\n",
    "    df_brand = pd.concat([df_brand_2021_to_2023, df_brand_2023_oct])\n",
    "    df_brand = pd.concat([df_brand, df_brand_2024])\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_brand['Date'] = pd.to_datetime(df_brand['Date'])\n",
    "    \n",
    "    return df_brand\n",
    "\n",
    "\n",
    "def get_model_brand_media_spend_impression(file_path):\n",
    "    \"\"\"\n",
    "    Processes and aggregates brand media spend and impression data.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with aggregated brand media spend and impression data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get concatenated data from the US_brand_media_spend_impression function\n",
    "    df_ = US_brand_media_spend_impression(file_path)\n",
    "    \n",
    "    # Initialize an empty DataFrame for the results\n",
    "    df_brand = pd.DataFrame(columns=['Date'])\n",
    "    \n",
    "    # Uncommented code for future use\n",
    "    # for ven in df_['Vendor'].unique():\n",
    "    #     temp = df_[df_['Vendor'] == ven]\n",
    "    #     var = temp['Advertising Type'].values[0] + \"_\" + ven\n",
    "    #     df_brand_grp = temp.groupby(['Date', 'Vendor', 'Advertising Type']).sum().reset_index()\n",
    "    #     df_brand_grp = df_brand_grp.drop(['Vendor', 'Advertising Type'], axis=1)\n",
    "    #     df_brand_grp = df_brand_grp.rename({'Impressions': var + \"_Impressions\", 'Spend': var + \"_Spend\"}, axis=1)\n",
    "    #     df_brand = pd.merge(df_brand, df_brand_grp, on='Date', how='outer')\n",
    "    \n",
    "    # for ven in df_['Advertising Type'].unique():\n",
    "    #     temp = df_[df_['Advertising Type'] == ven]\n",
    "    #     var = temp['Advertising Type'].values[0]\n",
    "    #     df_brand_grp = temp.groupby(['Date', 'Advertising Type']).sum().reset_index()\n",
    "    #     df_brand_grp = df_brand_grp.drop(['Advertising Type'], axis=1)\n",
    "    #     df_brand_grp = df_brand_grp.rename({'Impressions': var + \"_Impressions\", 'Spend': var + \"_Spend\"}, axis=1)\n",
    "    #     df_brand = pd.merge(df_brand, df_brand_grp, on='Date', how='outer')\n",
    "    \n",
    "    # Group by 'Date' and aggregate the sum of 'Impressions' and 'Spend'\n",
    "    df_brand_grp = df_.groupby(['Date']).sum().reset_index()\n",
    "    df_brand_grp = df_brand_grp.rename({'Impressions': \"Brand_Impressions\", 'Spend': \"Brand_Spend\"}, axis=1)\n",
    "    \n",
    "    # Merge aggregated data with the result DataFrame\n",
    "    df_brand = pd.merge(df_brand, df_brand_grp, on='Date', how='outer')\n",
    "    \n",
    "    return df_brand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Brand Health Measures - TWC Brand performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_brand_health_measures:\n",
    "\n",
    "- Reads brand health measures data from a CSV file.\n",
    "- Sets the 'Measure' column as the index and transposes the DataFrame so the measures become columns.\n",
    "- Resets the index and renames the index column to 'Date' for further processing.\n",
    "- Returns the processed DataFrame.\n",
    "\n",
    "### get_year_qtr:\n",
    "\n",
    "- Extracts the 'Year' and 'Quarter' from a 'Date' column formatted as 'YYYY-QX'.\n",
    "- Adds 'Year' and 'Quarter' columns to the DataFrame for easier analysis.\n",
    "- Returns the updated DataFrame with these new columns.\n",
    "\n",
    "### get_model_brand_health_measures:\n",
    "\n",
    "- Uses the **get_brand_health_measures** function to load and prepare brand health data.\n",
    "- Adds missing quarters ('2021-Q2', '2023-Q4', and '2024-Q2') to the dataset.\n",
    "- Extracts 'Year' and 'Quarter', sorts the data, and converts them into a datetime format.\n",
    "- Resamples the data to a daily frequency, forward-filling missing values.\n",
    "- Returns the final DataFrame with daily frequency for brand health measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "- Brand health measures 2021 - 2024.csv - Contains Brand Health Measures data from 2021 to 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand_health_measures(file_path):\n",
    "    \"\"\"\n",
    "    Reads and processes brand health measures data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the CSV file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the transposed and indexed brand health measures data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df_brand_health = pd.read_csv(file_path + \"Brand health measures 2021 - 2024.csv\")\n",
    "    \n",
    "    # Set 'Measure' column as index and transpose the DataFrame\n",
    "    df_brand_health.set_index('Measure', inplace=True)\n",
    "    df_brand_health = df_brand_health.T\n",
    "    \n",
    "    # Reset index and rename 'index' column to 'Date'\n",
    "    df_brand_health = df_brand_health.reset_index()\n",
    "    df_brand_health = df_brand_health.rename({'index': 'Date'}, axis=1)\n",
    "    \n",
    "    return df_brand_health\n",
    "\n",
    "\n",
    "def get_year_qtr(df):\n",
    "    \"\"\"\n",
    "    Extracts 'Year' and 'Quarter' from the 'Date' column and adds them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, containing a 'Date' column in 'YYYY-QX' format\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with 'Year' and 'Quarter' columns added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regular expression pattern for extracting year and quarter\n",
    "    pattern = re.compile(r'(\\d+)-Q(\\d+)')\n",
    "    \n",
    "    # Apply regex pattern to extract year and quarter into new columns\n",
    "    df[['Year', 'Quarter']] = df['Date'].apply(lambda x: pd.Series(pattern.match(x).groups()))\n",
    "    df['Quarter'] = df['Quarter'].astype(int)\n",
    "    df['Year'] = df['Year'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_model_brand_health_measures(file_path):\n",
    "    \"\"\"\n",
    "    Processes brand health measures data, adds missing quarters, and performs date resampling.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the CSV file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with daily frequency and forward-filled values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the initial brand health measures data\n",
    "    df_brand_qtr = get_brand_health_measures(file_path)\n",
    "    \n",
    "    # Append missing quarters to the DataFrame\n",
    "    df_brand_qtr = df_brand_qtr.append({'Date': '2021-Q2'}, ignore_index=True)\n",
    "    df_brand_qtr = df_brand_qtr.append({'Date': '2023-Q4'}, ignore_index=True)\n",
    "    df_brand_qtr = df_brand_qtr.append({'Date': '2024-Q2'}, ignore_index=True)\n",
    "    \n",
    "    # Extract 'Year' and 'Quarter' from 'Date' column\n",
    "    df_brand_qtr = get_year_qtr(df_brand_qtr)\n",
    "    \n",
    "    # Sort DataFrame by 'Year' and 'Quarter'\n",
    "    df_brand_qtr = df_brand_qtr.sort_values(['Year', 'Quarter'])\n",
    "    \n",
    "    # Convert 'Year' and 'Quarter' to a datetime index\n",
    "    df_brand_qtr['Date_New'] = pd.to_datetime(df_brand_qtr['Year'].astype(str) + 'Q' + df_brand_qtr['Quarter'].astype(str))\n",
    "    df_brand_qtr.set_index('Date_New', inplace=True)\n",
    "    \n",
    "    # Resample DataFrame to daily frequency and forward-fill missing values\n",
    "    df_brand_health_measures = df_brand_qtr.resample('D').ffill()\n",
    "    \n",
    "    # Reset index and rename 'Date_New' to 'Date'\n",
    "    df_brand_health_measures.reset_index(inplace=True)\n",
    "    df_brand_health_measures = df_brand_health_measures.drop('Date', axis=1)\n",
    "    df_brand_health_measures = df_brand_health_measures.rename({'Date_New': 'Date'}, axis=1)\n",
    "    df_brand_health_measures['Date'] = pd.to_datetime(df_brand_health_measures['Date'])\n",
    "    \n",
    "    return df_brand_health_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Social Engagement - Scoial Media User engagement data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### social_engagement_pre_processing_khoros:\n",
    "\n",
    "- Loads and processes social engagement data from Khoros.\n",
    "- Cleans unwanted rows and columns, normalizes platform names, converts engagement metrics to float, and formats dates.\n",
    "- Returns the processed Khoros data.\n",
    "\n",
    "### social_engagement_pre_processing_sprinklr:\n",
    "\n",
    "- Loads and processes social engagement data from Sprinklr.\n",
    "- Cleans and filters rows, normalizes platform names, converts metrics to float, and formats the date column.\n",
    "- Returns the processed Sprinklr data.\n",
    "\n",
    "### social_engagement_pre_processing:\n",
    "\n",
    "- Processes social engagement data from a general source.\n",
    "- Renames columns, normalizes platform names, converts engagement metrics to float, aggregates data by date and platform, and fills missing values.\n",
    "- Returns the processed data.\n",
    "\n",
    "### get_model_social_engagement:\n",
    "\n",
    "- Combines social engagement data from Khoros, Sprinklr, and the general source.\n",
    "- Filters data by date, concatenates the datasets, and processes data for each platform.\n",
    "- Aggregates and normalizes data, returning a DataFrame with combined social engagement data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- Social Engagements-2020-2023_Revised.xlsx - Contains Social Engagements data from  Khoros for 2020 to 2023 \n",
    "- spr_web_analyst_with_Impressions_available_after_January_2022.xlsx - Contains Social Engagements data from Sprinklr for 2022 to 2023 \n",
    "- social_engg.xlsx - Contains Social Engagements data from 2024-01-01 to 2024-05-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_engagement_pre_processing_khoros(file_path):\n",
    "    \"\"\"\n",
    "    Processes social engagement data from Khoros.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed social engagement data from Khoros\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the specified Excel sheet\n",
    "    df_soc_eng_khrs = pd.read_excel(file_path + \"Social Engagements-2020-2023_Revised.xlsx\",\n",
    "                        sheet_name='2020 - 2023 Oct 25th')\n",
    "    \n",
    "    # Remove unwanted rows and columns\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs[df_soc_eng_khrs['Outbound Post'] != 'Outbound Post']\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs[df_soc_eng_khrs['Impressions'] != 'Awareness']\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs[df_soc_eng_khrs['Total Engagements (SUM)'] != 'Consideration']\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs.drop(['Unnamed: 52'], axis=1)\n",
    "    \n",
    "    # Filter out rows with missing platform values\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs[~df_soc_eng_khrs['Platform'].isna()]\n",
    "    \n",
    "    # Normalize platform names\n",
    "    df_soc_eng_khrs['Platform'] = np.where(df_soc_eng_khrs['Platform'] == 'Tiktok', 'TikTok', df_soc_eng_khrs['Platform'])\n",
    "    df_soc_eng_khrs['Platform'] = np.where(df_soc_eng_khrs['Platform'] == 'Linkedin', 'LinkedIn', df_soc_eng_khrs['Platform'])\n",
    "    df_soc_eng_khrs['Platform'] = np.where(df_soc_eng_khrs['Platform'] == 'Youtube', 'YouTube', df_soc_eng_khrs['Platform'])\n",
    "    \n",
    "    # Convert engagement metrics to float\n",
    "    df_soc_eng_khrs['Estimated Clicks (SUM)'] = df_soc_eng_khrs['Estimated Clicks (SUM)'].astype('float')\n",
    "    df_soc_eng_khrs['Impressions'] = df_soc_eng_khrs['Impressions'].astype('float')\n",
    "    df_soc_eng_khrs['Total Engagements (SUM)'] = df_soc_eng_khrs['Total Engagements (SUM)'].astype('float')\n",
    "    \n",
    "    # Convert date column to datetime format\n",
    "    df_soc_eng_khrs['Date'] = pd.to_datetime(df_soc_eng_khrs['Date'])\n",
    "    \n",
    "    # Filter for Khoros data source\n",
    "    df_soc_eng_khrs = df_soc_eng_khrs[df_soc_eng_khrs['Source'] == 'Khoros']\n",
    "    \n",
    "    # Split and reformat date and time columns\n",
    "    df_soc_eng_khrs[['Date_Copy', 'Time_Copy']] = df_soc_eng_khrs[['Date', 'Time']]\n",
    "    df_soc_eng_khrs[['Date', 'Time']] = df_soc_eng_khrs['Date'].astype(str).str.split(' ', expand=True)\n",
    "    df_soc_eng_khrs['Date'] = pd.to_datetime(df_soc_eng_khrs['Date'])\n",
    "    \n",
    "    return df_soc_eng_khrs\n",
    "\n",
    "\n",
    "def social_engagement_pre_processing_sprinklr(file_path):\n",
    "    \"\"\"\n",
    "    Processes social engagement data from Sprinklr.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed social engagement data from Sprinklr\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the specified Excel file\n",
    "    df_soc_eng_spr = pd.read_excel(file_path + \"spr_web_analyst_with_Impressions_available_after_January_2022.xlsx\")\n",
    "    \n",
    "    # Remove unwanted rows and columns\n",
    "    df_soc_eng_spr = df_soc_eng_spr[df_soc_eng_spr['Outbound Post'] != 'Outbound Post']\n",
    "    df_soc_eng_spr = df_soc_eng_spr[df_soc_eng_spr['Impressions'] != 'Awareness']\n",
    "    df_soc_eng_spr = df_soc_eng_spr[df_soc_eng_spr['Total Engagements (SUM)'] != 'Consideration']\n",
    "    df_soc_eng_spr = df_soc_eng_spr[~df_soc_eng_spr['Platform'].isna()]\n",
    "    \n",
    "    # Normalize platform names\n",
    "    df_soc_eng_spr['Platform'] = np.where(df_soc_eng_spr['Platform'] == 'Tiktok', 'TikTok', df_soc_eng_spr['Platform'])\n",
    "    df_soc_eng_spr['Platform'] = np.where(df_soc_eng_spr['Platform'] == 'Linkedin', 'LinkedIn', df_soc_eng_spr['Platform'])\n",
    "    df_soc_eng_spr['Platform'] = np.where(df_soc_eng_spr['Platform'] == 'Youtube', 'YouTube', df_soc_eng_spr['Platform'])\n",
    "    \n",
    "    # Convert engagement metrics to float\n",
    "    df_soc_eng_spr['Estimated Clicks (SUM)'] = df_soc_eng_spr['Estimated Clicks (SUM)'].astype('float')\n",
    "    df_soc_eng_spr['Impressions'] = df_soc_eng_spr['Impressions'].astype('float')\n",
    "    df_soc_eng_spr['Total Engagements (SUM)'] = df_soc_eng_spr['Total Engagements (SUM)'].astype('float')\n",
    "    \n",
    "    # Convert date column to datetime format\n",
    "    df_soc_eng_spr['Date'] = pd.to_datetime(df_soc_eng_spr['Date'])\n",
    "    \n",
    "    # Filter for specific platforms and set data source\n",
    "    df_soc_eng_spr = df_soc_eng_spr[df_soc_eng_spr['Platform'].isin(['Facebook', 'LinkedIn', 'YouTube', 'Instagram', 'Twitter', 'TikTok'])]\n",
    "    df_soc_eng_spr['Source'] = 'Sprinklr'\n",
    "    \n",
    "    return df_soc_eng_spr\n",
    "\n",
    "\n",
    "def social_engagement_pre_processing(file_path):\n",
    "    \"\"\"\n",
    "    Processes social engagement data from a general source.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed social engagement data from the general source\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the specified Excel file\n",
    "    df_soc_eng_2024 = pd.read_excel(file_path + \"social_engg.xlsx\")\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    df_soc_eng_2024.rename(columns={'Post Reach (SUM)': 'Impressions', 'Channel': 'Platform'}, inplace=True)\n",
    "    \n",
    "    # Normalize platform names\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'TIKTOK_BUSINESS', 'TikTok', df_soc_eng_2024['Platform'])\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'LINKEDIN_COMPANY', 'LinkedIn', df_soc_eng_2024['Platform'])\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'YOUTUBE', 'YouTube', df_soc_eng_2024['Platform'])\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'INSTAGRAM', 'Instagram', df_soc_eng_2024['Platform'])\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'FBPAGE', 'Facebook', df_soc_eng_2024['Platform'])\n",
    "    df_soc_eng_2024['Platform'] = np.where(df_soc_eng_2024['Platform'] == 'TWITTER', 'Twitter', df_soc_eng_2024['Platform'])\n",
    "    \n",
    "    # Convert engagement metrics to float\n",
    "    df_soc_eng_2024['Estimated Clicks (SUM)'] = df_soc_eng_2024['Estimated Clicks (SUM)'].astype('float')\n",
    "    df_soc_eng_2024['Impressions'] = df_soc_eng_2024['Impressions'].astype('float')\n",
    "    df_soc_eng_2024['Total Engagements (SUM)'] = df_soc_eng_2024['Total Engagements (SUM)'].astype('float')\n",
    "    \n",
    "    # Convert 'PublishedTime' to date and fill missing values\n",
    "    df_soc_eng_2024['Date'] = pd.to_datetime(df_soc_eng_2024['PublishedTime']).dt.date\n",
    "    df_soc_eng_2024 = df_soc_eng_2024[df_soc_eng_2024['Platform'].isin(['Facebook', 'LinkedIn', 'YouTube', 'Instagram', 'Twitter', 'TikTok'])]\n",
    "    df_soc_eng_2024 = df_soc_eng_2024.fillna(0)\n",
    "    \n",
    "    # Aggregate data by date and platform\n",
    "    df_soc_eng_2024 = df_soc_eng_2024.groupby(['Date', 'Platform']).agg({\n",
    "        'Total Engagements (SUM)': 'sum',\n",
    "        'Estimated Clicks (SUM)': 'sum',\n",
    "        'Impressions': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return df_soc_eng_2024\n",
    "\n",
    "\n",
    "def get_model_social_engagement(file_path):\n",
    "    \"\"\"\n",
    "    Combines and processes social engagement data from Khoros, Sprinklr, and a general source.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with combined and processed social engagement data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get processed data from different sources\n",
    "    df_khrs = social_engagement_pre_processing_khoros(file_path)\n",
    "    df_spr = social_engagement_pre_processing_sprinklr(file_path)\n",
    "    date_cutoff = pd.to_datetime('2023-12-31', format='%Y/%m/%d')\n",
    "    \n",
    "    # Filter data by date\n",
    "    df_khrs = df_khrs[df_khrs['Date'] <= date_cutoff]\n",
    "    df_spr = df_spr[df_spr['Date'] <= date_cutoff]\n",
    "    \n",
    "    # Get and process additional social engagement data\n",
    "    df_socengg_2024 = social_engagement_pre_processing(file_path)\n",
    "    \n",
    "    # Concatenate data from different sources\n",
    "    df_socengg = pd.concat([df_spr, df_khrs], axis=0)\n",
    "    df_socengg = pd.concat([df_socengg, df_socengg_2024], axis=0)\n",
    "    \n",
    "    # Initialize result DataFrame\n",
    "    df_social_engagement = pd.DataFrame(columns=['Date'])\n",
    "    \n",
    "    # Normalize platform names\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'Tiktok', 'TikTok', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'Linkedin', 'LinkedIn', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'Youtube', 'YouTube', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'TIKTOK_BUSINESS', 'TikTok', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'LINKEDIN_COMPANY', 'LinkedIn', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'YOUTUBE', 'YouTube', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'INSTAGRAM', 'Instagram', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'FBPAGE', 'Facebook', df_socengg['Platform'])\n",
    "    df_socengg['Platform'] = np.where(df_socengg['Platform'] == 'TWITTER', 'Twitter', df_socengg['Platform'])\n",
    "    \n",
    "    # Process data for each platform\n",
    "    for plt in df_socengg['Platform'].unique():\n",
    "        temp = df_socengg[df_socengg['Platform'] == plt]\n",
    "        df_soc_eng_grp = temp.groupby(['Date', 'Platform'])[['Impressions',\n",
    "                                                     'Total Engagements (SUM)',\n",
    "                                                     'Estimated Clicks (SUM)']].sum().reset_index()\n",
    "        \n",
    "        # Drop the 'Platform' column and rename columns\n",
    "        df_soc_eng_grp = df_soc_eng_grp.drop(['Platform'], axis=1)\n",
    "        df_soc_eng_grp = df_soc_eng_grp.rename(columns={\n",
    "            col: \"SocialEng_\" + plt + \"_\" + col if col != 'Date' else col\n",
    "            for col in df_soc_eng_grp.columns\n",
    "        })\n",
    "        df_soc_eng_grp.columns = df_soc_eng_grp.columns.str.rstrip('(SUM)').str.strip()\n",
    "        df_soc_eng_grp.columns = df_soc_eng_grp.columns.str.replace(' ', '_')\n",
    "        \n",
    "        # Merge with the result DataFrame\n",
    "        df_social_engagement = pd.merge(df_social_engagement, df_soc_eng_grp, on='Date', how='outer')\n",
    "    \n",
    "    return df_social_engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Marketing Events - TWC Campaign related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_model_mkt_events:\n",
    "\n",
    "- Reads a CSV file containing campaign/vendor event data.\n",
    "- Converts the 'Start_Date' and 'End_Date' columns to datetime format.\n",
    "- For each event, it creates a date range between the start and end date and marks those dates with a Campaign_Flag of 1.\n",
    "- Filters the data based on a user-specified date range (ads_date_range).\n",
    "- Groups the data by date and concatenates campaign names (if there are multiple events on the same date).\n",
    "- Ensures all dates within the specified range are present, even if no event occurred on that day.\n",
    "- Fills missing values in 'Campaign_Flag' with 0 and in 'Campaign/Vendor(s)' with \"NA.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- CampaignVendor_Events.csv - Contains TWC campaign data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mkt_events(file_path, ads_date_range):\n",
    "    \"\"\"\n",
    "    Processes marketing events data from a CSV file and prepares it for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the CSV file\n",
    "    - ads_date_range: list, [start_date, end_date] specifying the date range for filtering\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed marketing events data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    df_mkt_events = pd.read_csv(file_path + \"CampaignVendor_Events.csv\")\n",
    "    \n",
    "    # Convert 'Start_Date' and 'End_Date' columns to datetime format\n",
    "    df_mkt_events['Start_Date'] = pd.to_datetime(df_mkt_events['Start_Date'], format=\"%d-%m-%Y\")\n",
    "    df_mkt_events['End_Date'] = pd.to_datetime(df_mkt_events['End_Date'], format=\"%d-%m-%Y\")\n",
    "    \n",
    "    # Initialize an empty DataFrame for storing campaign information\n",
    "    df_campaign = pd.DataFrame(columns=['Date', 'Campaign_Flag', 'Campaign/Vendor(s)'])\n",
    "    \n",
    "    # Iterate through each row to create a date range for each campaign\n",
    "    for idx, row in df_mkt_events.iterrows():\n",
    "        campaign_name = row['Campaign/Vendor(s)']\n",
    "        start_date = row['Start_Date']\n",
    "        end_date = row['End_Date']\n",
    "        \n",
    "        # Create a DataFrame with dates from start_date to end_date\n",
    "        df_date_range = pd.DataFrame(pd.date_range(start=start_date, end=end_date, freq='D'), columns=['Date'])\n",
    "        df_date_range['Campaign_Flag'] = 1\n",
    "        df_date_range['Campaign/Vendor(s)'] = campaign_name\n",
    "        \n",
    "        # Append the date range DataFrame to the main DataFrame\n",
    "        df_campaign = pd.concat([df_campaign, df_date_range], ignore_index=True)\n",
    "    \n",
    "    # Fill missing values with 0 and \"NA\"\n",
    "    df_campaign = df_campaign.fillna(0)\n",
    "    \n",
    "    # Filter DataFrame based on the provided date range\n",
    "    df_campaign = df_campaign[(df_campaign['Date'] >= ads_date_range[0]) & (df_campaign['Date'] <= ads_date_range[1])]\n",
    "    \n",
    "    # Group by 'Date' and aggregate the results\n",
    "    df_campaign = df_campaign.groupby('Date').agg({\n",
    "        'Campaign_Flag': 'max',\n",
    "        'Campaign/Vendor(s)': lambda x: ' ; '.join(x)\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create a DataFrame with all dates within the specified range\n",
    "    all_dates = pd.DataFrame(pd.date_range(start=ads_date_range[0], end=ads_date_range[1], freq='D'), columns=['Date'])\n",
    "    \n",
    "    # Merge with the all_dates DataFrame to include missing dates\n",
    "    df_campaign = pd.merge(all_dates, df_campaign, on='Date', how='left')\n",
    "    \n",
    "    # Fill missing values with 0 for 'Campaign_Flag' and \"NA\" for 'Campaign/Vendor(s)'\n",
    "    df_campaign['Campaign_Flag'] = df_campaign['Campaign_Flag'].fillna(0).astype(int)\n",
    "    df_campaign['Campaign/Vendor(s)'] = df_campaign['Campaign/Vendor(s)'].fillna(pd.NA)\n",
    "\n",
    "    return df_campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Critical Events - USA Critical Events data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_model_critical_events:\n",
    "\n",
    "- Reads a CSV file containing critical event information.\n",
    "- Converts the 'Start_Date' and 'End_Date' columns to datetime format.\n",
    "- For each critical event, it creates a date range between the start and end date, marking those dates with a Critical_Event_Flag of 1 and recording the event name.\n",
    "- Filters the data based on a user-specified date range (ads_date_range).\n",
    "- Groups the data by date, and concatenates event names (if multiple events occur on the same day).\n",
    "- Ensures all dates within the specified range are present, even if no event occurred on that day.\n",
    "- Fills missing values in 'Critical_Event_Flag' with 0 and in 'Event_Name' with \"NA.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- Critical_Events.csv - Contains USA Critical Events data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_critical_events(file_path, ads_date_range):\n",
    "    \"\"\"\n",
    "    Processes critical events data from a CSV file and prepares it for analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the CSV file\n",
    "    - ads_date_range: list, [start_date, end_date] specifying the date range for filtering\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed critical events data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    df_crit_events = pd.read_csv(file_path + \"Critical_Events.csv\")\n",
    "    \n",
    "    # Convert 'Start_Date' and 'End_Date' columns to datetime format\n",
    "    df_crit_events['Start_Date'] = pd.to_datetime(df_crit_events['Start_Date'], format=\"%d-%m-%Y\")\n",
    "    df_crit_events['End_Date'] = pd.to_datetime(df_crit_events['End_Date'], format=\"%d-%m-%Y\")\n",
    "    \n",
    "    # Initialize an empty DataFrame for storing critical event information\n",
    "    df_event = pd.DataFrame(columns=['Date', 'Critical_Event_Flag', 'Event_Name'])\n",
    "    \n",
    "    # Iterate through each row to create a date range for each critical event\n",
    "    for idx, row in df_crit_events.iterrows():\n",
    "        event_name = row['Critical_Event'].split(': ')[1]  # Extract event name after \": \"\n",
    "        start_date = row['Start_Date']\n",
    "        end_date = row['End_Date']\n",
    "        \n",
    "        # Create a DataFrame with dates from start_date to end_date\n",
    "        df_date_range = pd.DataFrame(pd.date_range(start=start_date, end=end_date, freq='D'), columns=['Date'])\n",
    "        df_date_range['Critical_Event_Flag'] = 1\n",
    "        df_date_range['Event_Name'] = event_name\n",
    "        \n",
    "        # Append the date range DataFrame to the main DataFrame\n",
    "        df_event = pd.concat([df_event, df_date_range], ignore_index=True)\n",
    "    \n",
    "    # Fill missing values with 0 for 'Critical_Event_Flag' and \"NA\" for 'Event_Name'\n",
    "    df_event = df_event.fillna(0)\n",
    "    \n",
    "    # Filter DataFrame based on the provided date range\n",
    "    df_event = df_event[(df_event['Date'] >= ads_date_range[0]) & (df_event['Date'] <= ads_date_range[1])]\n",
    "    \n",
    "    # Group by 'Date' and aggregate the results\n",
    "    df_event = df_event.groupby('Date').agg({\n",
    "        'Critical_Event_Flag': 'max',\n",
    "        'Event_Name': lambda x: ' ; '.join(x)\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create a DataFrame with all dates within the specified range\n",
    "    all_dates = pd.DataFrame(pd.date_range(start=ads_date_range[0], end=ads_date_range[1], freq='D'), columns=['Date'])\n",
    "    \n",
    "    # Merge with the all_dates DataFrame to include missing dates\n",
    "    df_event = pd.merge(all_dates, df_event, on='Date', how='left')\n",
    "    \n",
    "    # Fill missing values with 0 for 'Critical_Event_Flag' and \"NA\" for 'Event_Name'\n",
    "    df_event['Critical_Event_Flag'] = df_event['Critical_Event_Flag'].fillna(0).astype(int)\n",
    "    df_event['Event_Name'] = df_event['Event_Name'].fillna(pd.NA)\n",
    "\n",
    "    return df_event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Influencer Data - Influencer Social Media Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_model_influencer_data:\n",
    "\n",
    "- Reads an Excel file containing influencer data from a specified sheet.\n",
    "- Extracts three columns: 'Date', 'Influencer_Spend', and 'Influencer_Daily_Impressions'.\n",
    "- Converts the 'Date' column to datetime format for easier manipulation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data:\n",
    "\n",
    "- Influencer_Data.xlsx - Contains Influencer data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_influencer_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes influencer data from an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the Excel file\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with processed influencer data containing 'Date', 'Influencer_Spend', and 'Influencer_Daily_Impressions'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the influencer data from the specified sheet of the Excel file\n",
    "    df_influencer = pd.read_excel(file_path + \"Influencer_Data.xlsx\", sheet_name='Influencer_data')\n",
    "    \n",
    "    # Select relevant columns from the DataFrame\n",
    "    df_influencer = df_influencer[['Date', 'Influencer_Spend', 'Influencer_Daily_Impressions']]\n",
    "    \n",
    "    # Convert 'Date' column to datetime format\n",
    "    df_influencer['Date'] = pd.to_datetime(df_influencer['Date'])\n",
    "    \n",
    "    return df_influencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Merging all the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_model_data:\n",
    "\n",
    "**Load and Process Data:**\n",
    "\n",
    "- Product Visits: Loads product visits data and processes date information.\n",
    "- Organic Search: Loads Google organic search data and renames columns.\n",
    "- SEO Clicks: Loads SEO clicks data and renames columns.\n",
    "- Pricing and Installs: Loads and renames pricing and app install data.\n",
    "- User Acquisition: Loads user acquisition marketing data.\n",
    "- Brand Media Spend: Loads brand media spend and impression data.\n",
    "- Brand Health Measures: Loads brand health measures data.\n",
    "- Social Engagement: Loads social engagement data.\n",
    "- Marketing Events: Loads marketing events data and renames columns.\n",
    "- Critical Events: Loads critical events data.\n",
    "- Influencer: Loads influencer data\n",
    "\n",
    "**Merge Data:**\n",
    "\n",
    "- Merges all the processed data into a single DataFrame on the 'Date' column.\n",
    "\n",
    "**Filter and Finalize:**\n",
    "\n",
    "- Filters the final DataFrame by a specified date range.\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "- Returns the merged and processed DataFrame containing data from all sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(file_path):\n",
    "    \"\"\"\n",
    "    Retrieves and processes model data from various sources, merging them into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the directory containing the data files\n",
    "    \n",
    "    Returns:\n",
    "    - df_aligned_data: DataFrame, merged and processed data from all sources\n",
    "    \"\"\"\n",
    "    \n",
    "    # Visits Data - Product (Target Variable) - US All Product (Web-App) Visits\n",
    "    df_model_product_visits = get_model_product_visits(file_path)\n",
    "    df_model_product_visits['Date'] = pd.to_datetime(df_model_product_visits['Date'])\n",
    "    print(\"Product Visits\", \n",
    "          df_model_product_visits.shape, \n",
    "          df_model_product_visits['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_product_visits['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_product_visits['Date'].max() - df_model_product_visits['Date'].min())\n",
    "    \n",
    "   \n",
    "    # Organic Search - Impressions/Clicks: Google Platform\n",
    "    df_model_organic_search = get_organic_search_google(file_path)\n",
    "    df_model_organic_search = df_model_organic_search.rename(\n",
    "        {'Clicks': 'OrganicSearch_Google_Clicks',\n",
    "         'Impressions': 'OrganicSearch_Google_Impressions',\n",
    "         'CTR': 'OrganicSearch_Google_CTR',\n",
    "         'Position': 'OrganicSearch_Google_Position'}, \n",
    "        axis=1\n",
    "    )\n",
    "    print(\"Organic Search\", \n",
    "          df_model_organic_search.shape, \n",
    "          df_model_organic_search['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_organic_search['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_organic_search['Date'].max() - df_model_organic_search['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_model_product_visits,\n",
    "                             df_model_organic_search,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # SEO Clicks - For Organic Search Clicks Imputation for missing data\n",
    "    df_model_seo_clicks = seo_clicks(file_path)\n",
    "    df_model_seo_clicks = df_model_seo_clicks.rename(\n",
    "        {'Organic Search; United States; desktop': 'SEO_Clicks_OrganicSearch_Desktop',\n",
    "         'Organic Search; United States; mobile web': 'SEO_Clicks_OrganicSearch_MobileWeb',\n",
    "         'Desktop & Mobile SEO Clicks (Combined)': 'SEO_Clicks_OrganicSearch_Desktop_MobileWeb(Combined)'}, \n",
    "        axis=1\n",
    "    )\n",
    "    df_model_seo_clicks['Date'] = pd.to_datetime(df_model_seo_clicks['Date'])\n",
    "    print(\"SEO Clicks (Organic)\", \n",
    "          df_model_seo_clicks.shape, \n",
    "          df_model_seo_clicks['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_seo_clicks['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_seo_clicks['Date'].max() - df_model_seo_clicks['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_seo_clicks,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Pricing/App Installs (Android and iOS)\n",
    "    df_model_pricing_installs = pricing_aggregate(file_path)\n",
    "    df_model_pricing_installs = df_model_pricing_installs.rename(\n",
    "        columns={col: 'Pricing_' + col if col != 'Date' else col for col in df_model_pricing_installs.columns}\n",
    "    )\n",
    "    print(\"Pricing\", \n",
    "          df_model_pricing_installs.shape, \n",
    "          df_model_pricing_installs['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_pricing_installs['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_pricing_installs['Date'].max() - df_model_pricing_installs['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_pricing_installs,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # User Acquisition - Media Spend, Impression, and Clicks\n",
    "    df_model_mkt_media_spend = get_model_user_acquisition_mkt_media_spend(file_path)\n",
    "    df_model_mkt_media_spend['Date'] = pd.to_datetime(df_model_mkt_media_spend['Date'])\n",
    "    print(\"Media Spend Impression\", \n",
    "          df_model_mkt_media_spend.shape, \n",
    "          df_model_mkt_media_spend['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_mkt_media_spend['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_mkt_media_spend['Date'].max() - df_model_mkt_media_spend['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_mkt_media_spend,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Brand - Media Spend, Impression (US Brand Basis)\n",
    "    df_model_brand_media_spend_impression = get_model_brand_media_spend_impression(file_path)\n",
    "    print(\"Brand Media Spend Impression (US Brand Basis)\", \n",
    "          df_model_brand_media_spend_impression.shape, \n",
    "          df_model_brand_media_spend_impression['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_brand_media_spend_impression['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_brand_media_spend_impression['Date'].max() - df_model_brand_media_spend_impression['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_brand_media_spend_impression,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Brand Health Measures\n",
    "    df_model_brand_health_measures = get_model_brand_health_measures(file_path)\n",
    "    df_model_brand_health_measures = df_model_brand_health_measures[df_model_brand_health_measures['Date'] <= df_aligned_data['Date'].max()]\n",
    "    print(\"Brand Measure\", \n",
    "          df_model_brand_health_measures.shape, \n",
    "          df_model_brand_health_measures['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_brand_health_measures['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_brand_health_measures['Date'].max() - df_model_brand_health_measures['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_brand_health_measures,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Social Engagement\n",
    "    df_model_social_engagemnet = get_model_social_engagement(file_path)\n",
    "    print(\"Social Engagement\", \n",
    "          df_model_social_engagemnet.shape, \n",
    "          df_model_social_engagemnet['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_social_engagemnet['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_social_engagemnet['Date'].max() - df_model_social_engagemnet['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_social_engagemnet,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "    \n",
    "    # Marketing Events\n",
    "    df_model_mkt_events = get_model_mkt_events(file_path, [df_aligned_data['Date'].min(), df_aligned_data['Date'].max()])\n",
    "    df_model_mkt_events = df_model_mkt_events.rename(\n",
    "        columns={col: 'EventsCamp/Vend_' + col if col != 'Date' else col for col in df_model_mkt_events.columns}\n",
    "    )\n",
    "    print(\"Marketing Events\", \n",
    "          df_model_mkt_events.shape, \n",
    "          df_model_mkt_events['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_mkt_events['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_mkt_events['Date'].max() - df_model_mkt_events['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_mkt_events,\n",
    "                             on='Date',\n",
    "                             how='left')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Critical Events\n",
    "    df_model_critical_events = get_model_critical_events(file_path, [df_aligned_data['Date'].min(), df_aligned_data['Date'].max()])\n",
    "    print(\"Critical Events\", \n",
    "          df_model_critical_events.shape, \n",
    "          df_model_critical_events['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_critical_events['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_critical_events['Date'].max() - df_model_critical_events['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_critical_events,\n",
    "                             on='Date',\n",
    "                             how='left')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "\n",
    "\n",
    "    # Influencer Data\n",
    "    df_model_influencer_data = get_model_influencer_data(file_path)\n",
    "    print(\"Influencer Data\", \n",
    "          df_model_influencer_data.shape, \n",
    "          df_model_influencer_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_influencer_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_model_influencer_data['Date'].max() - df_model_influencer_data['Date'].min())\n",
    "    \n",
    "    df_aligned_data = pd.merge(df_aligned_data,\n",
    "                             df_model_influencer_data,\n",
    "                             on='Date',\n",
    "                             how='outer')\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "\n",
    "\n",
    "\n",
    "    # Filter data by date range\n",
    "    df_aligned_data = df_aligned_data[(df_aligned_data['Date'] >= '2021-09-01') & (df_aligned_data['Date'] <= '2023-12-31')]\n",
    "    print(\"ADS\", \n",
    "          df_aligned_data.shape, \n",
    "          df_aligned_data['Date'].min().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max().strftime(\"%d-%m-%Y\"), \n",
    "          df_aligned_data['Date'].max() - df_aligned_data['Date'].min())\n",
    "    \n",
    "    \n",
    "    return df_aligned_data\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
