{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### health_measure_pre_processing:\n",
    "- This function handles missing data for health-related measures in a DataFrame by filling missing values using two different methods\n",
    "\n",
    "- **Backward Fill (bfill):** Columns like 'Positive impact on community' and 'Positive impact on well being' will have missing values filled using the next valid observation in the column.\n",
    "\n",
    "- **Forward Fill (ffill):** A wider set of columns (e.g., 'Net Favorability', 'Likelihood to recommend', 'NPS', etc.) will have missing values filled using the previous valid observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Health Measures Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def health_measure_pre_processing(df_):\n",
    "    \"\"\"\n",
    "    Pre-processes brand health measures by filling missing values \n",
    "    using backward and forward filling methods.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, brand health data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, processed brand health data\n",
    "    \"\"\"\n",
    "\n",
    "    # Columns to fill missing values using backward fill\n",
    "    hm_back_fill_var = ['Positive impact on community', 'Positive impact on well being']\n",
    "\n",
    "    # Columns to fill missing values using forward fill\n",
    "    hm_forward_fill_var = [\n",
    "        'Net Favorability', 'Likelihood to recommend', 'Net Trust', 'Realibality', \n",
    "        'Accuracy', 'NPS', 'Usage', 'Preference', 'Seen as experts', \n",
    "        'Positive impact on community', 'Positive impact on well being'\n",
    "    ]\n",
    "\n",
    "    # Apply backward fill to specified columns\n",
    "    df_[hm_back_fill_var] = df_[hm_back_fill_var].fillna(method='bfill')\n",
    "\n",
    "    # Apply forward fill to specified columns\n",
    "    df_[hm_forward_fill_var] = df_[hm_forward_fill_var].fillna(method='ffill')\n",
    "\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organic Search Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### organic_search_pre_processing:\n",
    "\n",
    "- **Date Conversion:** The 'Date' column is converted to a proper datetime format using pd.to_datetime() to ensure that the data can be time-series processed.\n",
    "\n",
    "- **CTR Calculation for Imputation:** It calculates a consistent Click-Through Rate (CTR) for imputation by dividing the total clicks by total impressions from non-missing values. This CTR is used later to estimate missing values.\n",
    "\n",
    "- **Imputation of Missing Clicks:** Missing values in the 'OrganicSearch_Google_Clicks' column are filled using data from another column ('SEO_Clicks_OrganicSearch_Desktop_MobileWeb(Combined)').\n",
    "\n",
    "- **Imputation of Missing Impressions:** Missing values in the 'OrganicSearch_Google_Impressions' column are estimated by dividing the already filled 'OrganicSearch_Google_Clicks' by the previously calculated CTR.\n",
    "\n",
    "- **Imputation of Missing Positions:** Missing values in 'OrganicSearch_Google_Position' are filled using the median of the non-missing values in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organic_search_pre_processing(df_):\n",
    "    \"\"\"\n",
    "    Pre-processes organic search data by handling missing values \n",
    "    and converting the 'Date' column to datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, organic search data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, processed organic search data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert 'Date' column to datetime\n",
    "    df_['Date'] = pd.to_datetime(df_['Date'])\n",
    "\n",
    "    # Calculate consistent CTR for imputation\n",
    "    consisten_CTR_for_imputation = round(\n",
    "        df_[df_['OrganicSearch_Google_Clicks'].notna()]['OrganicSearch_Google_Clicks'].sum() / \n",
    "        df_[df_['OrganicSearch_Google_Impressions'].notna()]['OrganicSearch_Google_Impressions'].sum(), \n",
    "        4\n",
    "    )\n",
    "\n",
    "    # Fill missing 'OrganicSearch_Google_Clicks' using 'SEO_Clicks_OrganicSearch_Desktop_MobileWeb(Combined)'\n",
    "    df_.loc[df_['OrganicSearch_Google_Clicks'].isna(), 'OrganicSearch_Google_Clicks'] = df_.loc[\n",
    "        df_['OrganicSearch_Google_Clicks'].isna(), 'SEO_Clicks_OrganicSearch_Desktop_MobileWeb(Combined)'\n",
    "    ]\n",
    "\n",
    "    # Impute missing 'OrganicSearch_Google_Impressions' using the calculated CTR\n",
    "    df_['OrganicSearch_Google_Impressions'] = np.where(\n",
    "        df_['OrganicSearch_Google_Impressions'].isna(), \n",
    "        round(df_['OrganicSearch_Google_Clicks'] / consisten_CTR_for_imputation), \n",
    "        df_['OrganicSearch_Google_Impressions']\n",
    "    )\n",
    "\n",
    "    # Impute missing 'OrganicSearch_Google_Position' with the median value of non-missing data\n",
    "    df_['OrganicSearch_Google_Position'] = np.where(\n",
    "        df_['OrganicSearch_Google_Position'].isna(), \n",
    "        np.median(df_[df_['OrganicSearch_Google_Position'].notna()]['OrganicSearch_Google_Position']), \n",
    "        df_['OrganicSearch_Google_Position']\n",
    "    )\n",
    "\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### social_media_pre_processing:\n",
    "\n",
    "- **Date Conversion:** It converts the 'Date' column to a datetime format to facilitate time-based filtering and operations.\n",
    "\n",
    "- **Mean Calculation for Imputation:** A date range (from September 1, 2021, to December 31, 2023) is used to calculate the mean values for three LinkedIn metrics: Impressions, Total Engagements, and Estimated Clicks. These means are calculated only from non-missing values within the specified date range.\n",
    "\n",
    "- **Date Range for Imputation:** The function defines another date range (April 1, 2023, to December 31, 2023) within which it will impute missing values using the calculated means.\n",
    "\n",
    "- **Imputation of Missing Values:** Missing values in the 'SocialEng_LinkedIn_Impressions', 'SocialEng_LinkedIn_Total_Engagements', and 'SocialEng_LinkedIn_Estimated_Clicks' columns during the imputation date range are filled with the respective mean values calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_media_pre_processing(df_):\n",
    "    \"\"\"\n",
    "    Pre-processes social media data by handling missing values for LinkedIn-related metrics \n",
    "    (Impressions, Total Engagements, and Estimated Clicks) using mean imputation for a specific date range.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, social media engagement data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, processed social media engagement data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert 'Date' column to datetime\n",
    "    df_['Date'] = pd.to_datetime(df_['Date'])\n",
    "\n",
    "    # Define the date range for calculating mean values\n",
    "    start_date = pd.to_datetime('2021-09-01')\n",
    "    end_date = pd.to_datetime('2023-12-31')\n",
    "    date_mask = (df_['Date'] >= start_date) & (df_['Date'] <= end_date)\n",
    "\n",
    "    # Calculate mean values for LinkedIn-related metrics\n",
    "    mean_impressions = df_[date_mask & df_['SocialEng_LinkedIn_Impressions'].notna()]['SocialEng_LinkedIn_Impressions'].mean()\n",
    "    mean_total_engagements = df_[date_mask & df_['SocialEng_LinkedIn_Total_Engagements'].notna()]['SocialEng_LinkedIn_Total_Engagements'].mean()\n",
    "    mean_estimated_clicks = df_[date_mask & df_['SocialEng_LinkedIn_Estimated_Clicks'].notna()]['SocialEng_LinkedIn_Estimated_Clicks'].mean()\n",
    "\n",
    "    # Define the date range for imputing missing values\n",
    "    impute_start_date = pd.to_datetime('2023-04-01')\n",
    "    impute_end_date = pd.to_datetime('2023-12-31')\n",
    "    impute_mask = (df_['Date'] >= impute_start_date) & (df_['Date'] <= impute_end_date)\n",
    "\n",
    "    # Impute missing values for LinkedIn metrics using the calculated mean values\n",
    "    df_['SocialEng_LinkedIn_Impressions'] = np.where(\n",
    "        impute_mask & df_['SocialEng_LinkedIn_Impressions'].isna(), \n",
    "        mean_impressions, \n",
    "        df_['SocialEng_LinkedIn_Impressions']\n",
    "    )\n",
    "\n",
    "    df_['SocialEng_LinkedIn_Total_Engagements'] = np.where(\n",
    "        impute_mask & df_['SocialEng_LinkedIn_Total_Engagements'].isna(), \n",
    "        mean_total_engagements, \n",
    "        df_['SocialEng_LinkedIn_Total_Engagements']\n",
    "    )\n",
    "\n",
    "    df_['SocialEng_LinkedIn_Estimated_Clicks'] = np.where(\n",
    "        impute_mask & df_['SocialEng_LinkedIn_Estimated_Clicks'].isna(), \n",
    "        mean_estimated_clicks, \n",
    "        df_['SocialEng_LinkedIn_Estimated_Clicks']\n",
    "    )\n",
    "\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Empty Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop_empty_fields:\n",
    "\n",
    "- **Identify Columns with All Missing Values:** It checks each column in the DataFrame to see if all its values are NaN (missing). Columns that meet this condition are identified and stored in cols_v1.\n",
    "\n",
    "- **Identify Columns with a Sum of Zero:** The function calculates the sum of each column. If a column's sum is zero (which might indicate that all values are zero), the column is identified and stored in cols_v2.\n",
    "\n",
    "- **Combine and Drop Columns:** The function combines the columns identified in cols_v1 and cols_v2 into a single list, cols, and removes these columns from the DataFrame.\n",
    " - **Output:** If any columns are dropped, it prints the names of the dropped columns for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_fields(df_):\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame that are entirely empty or have a sum of zero.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, the input data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, with columns dropped if they contain only NaN values or sum to zero\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify columns with all NaN values\n",
    "    cols_v1 = list(df_.columns[df_.isna().sum() == df_.shape[0]])\n",
    "    \n",
    "    # Identify columns where the sum is zero\n",
    "    column_sums = df_.sum()\n",
    "    cols_v2 = column_sums[column_sums == 0].index.tolist()\n",
    "    \n",
    "    # Combine both lists of columns to be dropped\n",
    "    cols = list(set(cols_v1 + cols_v2))\n",
    "    \n",
    "    # Drop columns if any are identified\n",
    "    if cols:\n",
    "        df_ = df_.drop(cols, axis=1)\n",
    "        print('Columns dropped due to null/zero fields:', cols)\n",
    "    \n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop_unnecessary_columns:\n",
    "\n",
    "- **Print Statements:** The function includes print statements to explain why certain columns are being dropped, providing context for the decisions.\n",
    "\n",
    "- **Define Columns to Drop:**\n",
    "    - Date Variables: Columns related to date information.\n",
    "    - Organic Search Variables: Columns related to CTR and SEO clicks.\n",
    "    - Partner Visits Variables: Columns related to partner visit data.\n",
    "    - Goal Visits Variables: Columns related to goal visits.\n",
    "\n",
    "- **Combine and Drop Columns:** All specified columns are combined into a single list and removed from the DataFrame using the drop method.\n",
    "\n",
    "- **Output:** The function returns the DataFrame with the unnecessary columns removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnecessary_columns(df_):\n",
    "    \"\"\"\n",
    "    Drops unnecessary columns from the DataFrame based on predefined categories.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, the input data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, with unnecessary columns dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define columns related to dates\n",
    "    date_var = ['Year', 'Quarter']\n",
    "    \n",
    "    # Print statements explaining the dropped variables\n",
    "    print(\"Dropping CTR and SEO clicks as organic search imputation is complete\")\n",
    "    print(\"Dropping Platform/Product visits as overall visits are considered, being the target variable\")\n",
    "    print(\"Dropping Overall/Platform/Product *Goal* visits, the variable is the total goal visits for that date; data is sparse\")\n",
    "    \n",
    "    # Define various categories of columns to drop\n",
    "    organic_search_var = ['OrganicSearch_Google_CTR', 'SEO_Clicks_OrganicSearch_Desktop_MobileWeb(Combined)']\n",
    "    platform_installs_var = [\n",
    "        'Pricing_Android_Installs', \n",
    "        'Pricing_iOS_Installs'\n",
    "    ]\n",
    "    goal_visits_var = [\n",
    "        'Apps_TWC Universal Android 4G+_Goal', \n",
    "        'Apps_TWC Universal iOS 4G+_Goal', \n",
    "        'MobileWeb_TWC Mobile Web_Goal', \n",
    "        'Web_TWC Web_Goal', \n",
    "        'Overall_Product_Goal'\n",
    "    ]\n",
    "    \n",
    "    # Combine all columns to drop into one list\n",
    "    drop_var = date_var + organic_search_var + platform_installs_var + goal_visits_var\n",
    "    \n",
    "    # Drop the specified columns from the DataFrame\n",
    "    df_ = df_.drop(drop_var, axis=1)\n",
    "    \n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform_pre_processing:\n",
    "\n",
    "- **Function Purpose:** The function orchestrates multiple preprocessing steps to clean and prepare the data for further analysis or modeling.\n",
    "\n",
    "- **Brand Health Imputation:** Calls **health_measure_pre_processing** to handle missing values in brand health metrics.\n",
    "\n",
    "- **Organic Search Imputation:** Calls **organic_search_pre_processing** to impute missing values in organic search data based on calculated metrics.\n",
    "\n",
    "- **Social Media LinkedIn Imputation:** Calls **social_media_pre_processing** to impute missing LinkedIn engagement metrics using mean values.\n",
    "\n",
    "- **Drop Empty Fields:** Calls **rop_empty_fields** to remove columns that are entirely empty or have all zero values.\n",
    "\n",
    "- **Drop Unnecessary Columns:** Calls **drop_unnecessary_columns** to remove columns that are no longer relevant based on predefined criteria.\n",
    "\n",
    "- **Returns:** The function returns the DataFrame with all preprocessing steps applied, resulting in cleaner and more relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pre_processing(df_):\n",
    "    \"\"\"\n",
    "    Performs a series of preprocessing steps on the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df_: DataFrame, the input data\n",
    "\n",
    "    Returns:\n",
    "    - df_: DataFrame, with all preprocessing steps applied\n",
    "    \"\"\"\n",
    "    \n",
    "    # Print and apply Brand Health imputation\n",
    "    print('***** Brand Health - Imputation *****')\n",
    "    df_ = health_measure_pre_processing(df_)\n",
    "    \n",
    "    # Print and apply Organic Search imputation\n",
    "    print('***** Organic Search - Imputation *****')\n",
    "    df_ = organic_search_pre_processing(df_)\n",
    "    \n",
    "    # Print and apply Social Media LinkedIn imputation\n",
    "    print('***** Social Media - LinkedIn - Imputation *****')\n",
    "    df_ = social_media_pre_processing(df_)\n",
    "    \n",
    "    # Print and check for variables with no data, then drop empty fields\n",
    "    print('***** Checking variables which do not have any data *****')\n",
    "    df_ = drop_empty_fields(df_)\n",
    "    \n",
    "    # Print and drop unnecessary columns\n",
    "    print('***** Dropping unnecessary columns *****')\n",
    "    df_ = drop_unnecessary_columns(df_)\n",
    "    \n",
    "    return df_\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
